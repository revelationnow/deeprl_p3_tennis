{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Collaborative Competiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DDPG based agent was used to control the two players. DDPG is a stochastic policy gradient method and is also an actor-critic method in a sense. It uses the actor network to estimate the continuous action, given the input state and the critic network to estimate the Q value given a state and action input. DDPG is also an off-policy algorithm, which implies use of the concept of trajectories for the purpose of calculating the rewards for future actions. This is implemented through the target networks in the code. This enables the behaviour generation to be somewhat stable and not constantly updated. In the code, the target networks have identical architectures to the corresponding local networks. As expected the target networks need to be updated at some point so that they don't diverge dramatically from the local networks which output the behaviour of the agent. Currently this is done every learning step in the code. DDPG has been known to be unstable in the absense of a replay buffer. Here both a regular replay buffer that sample experiences from past actions uniformly at random and a TD error prioritized replay buffer have been tried. The prioritized replay buffer appeared to overfit early in the training process, limiting the average score from going beyond 0.1, while the regular uniformly randomly sampled replay buffer didn't have the overfitting problem, achieving the target score.\n",
    "\n",
    "The code can be found the Iron Python notebook called Tennis_Final_Solution.ipynb\n",
    "\n",
    "The saved weights are located in the Final_Weights directory\n",
    "\n",
    "To see the saved weights in action, you can run the Test_Agents.ipynb notebook\n",
    "\n",
    "The framework was adapted from the previous project. The neural network for the actor consisted of a single 128 neuron layer connected to the output layer. The critic consisted of 3 hidden layers of 128, 64 and 32 units each. This architecture was based on some discussions and multiple iterations.\n",
    "The other hyper parameters are mentioned below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)      # replay buffer size\n",
    "BATCH_SIZE = 1024        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 3e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0      # L2 weight decay\n",
    "LATERAL_LIMIT = 1\n",
    "JUMP_LIMIT_MAX = 1\n",
    "JUMP_LIMIT_MIN = 0\n",
    "ACTOR_FC1_UNITS=128\n",
    "ACTOR_FC2_UNITS=64 # Not used\n",
    "ACTOR_FC3_UNITS=32 # Not used\n",
    "\n",
    "CRITIC_FC1_UNITS=128\n",
    "CRITIC_FC2_UNITS=64\n",
    "CRITIC_FC3_UNITS=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During implementation, a prioritized replay buffer was attempted, however it seemed to get stuck at an average score of 0.1, leading to the belief that a prioritized replay buffer is probably more valuable after an agent has been trained sufficiently. The results here are presented from a regular replay buffer, where the agent took 8166 episodes to reach the target. This number might appear large, however there is some variance in this. In previous runs, the agent was able to achieve the target score in as low as 1500 cycles, so some randomization effects are playing a role. Additionally, when changing the random seed, there were instances when the agent didn't achieve the score at all in 10,000 episodes. In addition to achieving the target score, the agent's maximum score was 2.2, which is what the weights are saved for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[regular_result_image]: ./Training_Result.png \"Regular Replay Training result\"\n",
    "\n",
    "Below is an image of the plot of average and maximum scores using a regular replay buffer:\n",
    "![Regular Buffer Training Result][regular_result_image]\n",
    "<center> Regular Replay Buffer </center>\n",
    "\n",
    "[pri_replay_result_image]: ./Pri_replay_training_result.png \"Prioritized Replay Training result\"\n",
    "Below is an image of the plot of average and maximum scores using a prioritized replay buffer:\n",
    "![Prioritized Replay Buffer][pri_replay_result_image]\n",
    "<center> Prioritized Replay Buffer </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was slow on our machine, so we limited the agent to train once every other timestep. A framework was created to enable switching between different profiles of testing easily. These profiles included:\n",
    "1. Completely Separate agents training independently with separate actor and critic models\n",
    "2. Agents sharing the replay buffer, but with separate action and critic models\n",
    "3. Agents sharing the replay buffer and actor models, but different critic models\n",
    "4. Agents sharing the replay buffer and critic models but different actor models\n",
    "5. Agents sharing all the above.\n",
    "\n",
    "Finally the solution was achieved using the 5th configuration with agents sharing all the above, the replay buffer, the actor model and the critic model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "One of the primary comments online seems to be that PPO has significant advantages over DDPG, and that is something I would definitely want to try next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video of Agents playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a link to the agents playing against each other using the saved weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=hrtIdekQtw8\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/hrtIdekQtw8/0.jpg\" \n",
    "alt=\"Demo of the result\" width=\"240\" height=\"180\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
